Google软件测试之道

2018-10-19 20:24:35

```
当有人来问我，Google 成功的关键是什么，我的第一个建议就是，不要招聘太多的测试人员。
Google在测试人员如此缺乏的情况下，是如何应对的呢？简单地说，在Google，写代码的开发人员也承担了质量的重任。质量从来就不仅仅是一些测试人员的问题。在Google，每个写代码的开发者本身就是测试者，质量在名义上也由这样的开发测试组合共同承担，如图1.1所示。在Google，谈论开发测试比（译注：这里指在人员数量上，开发和测试的比率）就像讨论太阳表面的空气质量一样，这本身没有任何意义。如果你是一名工程师，那么你同时也是一名测试人员。如果在你的职位头衔上有测试的字样，你的任务就是怎样使那些头衔上没有测试的人可以更好地去做测试。
```

2018-10-09 14:16:18

    质量不是被测试出来的

2018-10-09 14:16:30


    质量不等于测试。当你把开发过程和测试放到一起，就像在搅拌机里混合搅拌那样，直到不能区分彼此的时候，你就得到了质量。

2018-10-09 14:17:28


    SET是SWE在代码库上的合作伙伴，与增加功能性代码或提高性能的代码的SWE相比，SET更加关注于质量的提升和测试覆盖率的增加。SET写代码的目的是可以让SWE测试自己的功能。


2018-10-09 14:17:54
    
    TE 把用户放在第一位来思考。TE 组织整体质量实践，分析解释测试运行结果，驱动测试执行，构建端到端的自动化测试。

2018-10-09 14:19:04

    Google 经常在最初的版本里只包含最基本的可用功能，然后在后继的快速迭代的过程中得到内部和外部用户的反馈，而且在每次迭代的过程中都非常注重质量。一个产品在发布给用户使用之前，一般都要经历金丝雀版本、开发版本、测试版本、beta或正式发布版本。


2018-10-09 14:20:31
    
    小型测试涵盖单一的代码段，一般运行在完全虚假实现（fake）的环境里。中型测试涵盖多个模块且重点关注在模块之间的交互上，一般运行在虚假实现（fake）环境或真实环境中。大型测试涵盖任意多个模块，一般运行在真实的环境中，并使用真正的用户数据与资源。

2018-10-09 14:21:08
    
    对于所有的三种类型测试，当然更倾向于前者。如果能够自动化，并不需要人脑的智睿与直觉来判断，那就应该以自动化的方式实现。

2018-10-19 20:25:16

```
编写功能代码和编写测试代码在思维方式上有着很大的不同。
在这样乌托邦式（译注：乌托邦是一个理想的群体和社会的构想，名字由托马斯·摩尔的《乌托邦》一书中所写的完全理性的共和国“乌托邦”而来，意指理想完美的境界）的理想开发过程中，众多的功能开发人员（译注：feature developer）和测试开发人员（译注：test developer）需要通力合作，共同为打造同一款产品而努力。在我们假想的完美理想情况下，产品的每一个功能都对应一个开发人员，整个产品则配备一定数量的测试开发人员。测试开发人员通过使用测试工具与框架帮助功能开发人员解决特定的单元测试问题，而这些问题如果只是由功能开发人员独自完成，则会消耗掉他们许多的精力。
功能开发人员在编写功能代码的时候，测试开发人员编写测试代码，但我们还需要第三种角色，一个关心真正用户的角色。显然在我们理想化的乌托邦测试世界里，这个工作应该由第三种工程师来完成，既不是功能开发人员，也不是测试开发人员。我们把这个新角色称为用户开发人员（译注：user developer）。他们需要解决的主要问题是面向用户的任务，包括用例（use case）、用户故事、用户场景、探索式测试等。用户开发人员关心这些功能模块如何集成在一起成为一个完整的整体，他们主要考虑系统级别的问题，通常情况下都会从用户角度出发，验证独立模块集成在一起之后是否对最终用户产生价值。
这就是我们眼中软件开发过程的乌托邦理想模式，三种开发角色在可用性和可靠性方面分工合作，达到完美。每个角色专门处理重要的事情，相互之间又可以平等地合作。
```

2018-10-09 14:23:22

    Google的SWE是功能开发人员；Google的SET是测试开发人员；Google的TE是用户开发人员。

2018-10-19 20:25:58

```
Google 在平台方面有特定的目标，就是保持简单且统一。开发工作机和生产环境的机器都保持统一的Linux发行版本；一套集中控制的通用核心库；一套统一的通用代码、构建和测试基础设施；每个核心语言只有一个编译器；与语言无关的通用打包规范；文化上对这些共享资源的维护表示尊重且有激励。
使用统一的运行平台和相同的代码库，持续不断地在构建系统中打包（译注：打包是一个过程，包括将源代码编译成二进制文件，然后再把二进制文件统一封装在一个linux rpm包里面），这可以简化共享代码的维护工作。构建系统要求使用统一的打包规范，这个打包规范与项目特定的编程语言无关，与团队是否使用C++、Python或Java也都无关。大家使用同样的“构建文件”来打包生成二进制文件。
一个版本在构建的时候需要指定构建目标，这个构建目标（可以是公共库、二进制文件或测试套件）由许多源文件编译链接产生。下面是整体流程。
（1）针对某个服务，在一个或多个源代码文件中编写一类或一系列功能函数，并保证所有代码可以编译通过。
（2）把这个新服务的构建目标设定为公共库。
（3）通过调用这个库的方式编写一套单元测试用例，把外部重要依赖通过 mock 模拟实现。对于需要关注的代码路径，使用最常见的输入参数来验证。
（4）为单元测试创建一个测试构建目标。
（5）构建并运行测试目标，做适当的修改调整，直到所有的测试都运行成功。
（6）按要求运行静态代码分析工具，确保遵守统一的代码风格，且通过一系列常见问题的静态扫描检测。
（7）提交代码申请代码审核（后面对代码审核会做更多详细说明），根据反馈再做适当的修改，然后运行所有的单元测试并保证顺利通过。
产出将是两个配套的构建目标：库构建目标和测试构建目标。库构建目标是需要新发布的公共库、测试构建目标用以验证新发布的公共库是否满足需求。注意：在Google许多开发人员使用“测试驱动开发”的模式，这意味着步骤（3）会在步骤（1）和步骤（2）之前进行。
对于规模更大的服务，通过链接编译持续新增的代码，构建目标也会逐渐变大，直到整个服务全部构建完成。在这个时候，会产生二进制构建目标，其由包含主入口main函数文件和服务库链接在一起构成。现在，你完成了一个Google产品，它由三部分组成：一个经过良好测试的独立库、一个在可读性与可复用性方面都不错的公共服务库（这个服务库中还包含另外一套支持库，可以用来创建其他的服务）、一套覆盖所有重要构建目标的单元测试套件。
一个典型的Google产品由许多服务组成，所有产品团队都希望一个SWE负责对应一个服务。这意味着每个服务都可以并行地构建、打包和测试，一旦所有的服务都完成了，他们会在一个最终的构建目标里一起集成。为了保证单独的服务可以并行地开发，服务之间的接口需要在项目的早期就确定下来。这样，开发者会依赖在协商好的接口上，而不是依赖在需要开发的特定库上。为了不耽搁服务级别之间的早期测试，这些接口一般都不会真正实现，而只是做一个虚假的实现。
SET 会参与到许多测试目标的构建之中，并指出哪些地方需要小型测试。在多个构建目标集成在一起，形成规模更大应用程序的构建目标时，SET需要加速他们的工作，开始做一些更大规模的集成测试。在一个单独的库构建目标中，需要运行几乎所有的小型测试（由SWE编写，所有支持这个项目的SET都会给予帮助）。当构建目标日益增大时，SET也会参与到中大型测试的编写之中去。
在构建目标的增长到一定规模时，针对功能集成的小型测试会成为回归测试的一部分。如果一个测试用例，本应该运行通过，但如果运行失败，也会报一个测试用例的 bug。这个针对测试用例的bug和针对功能的bug没有任何区别。测试就是功能的一部分，问题较多的测试就是功能性 bug，一定要得到修复。这样才可以保证新增的功能不会把已有功能损坏掉，任何代码的修改都不会导致测试本身的失败。
在所有的这些活动中，SET 始终是核心参与者。他们在开发人员不知道哪些地方需要单元测试的时候可以明确指出。他们同时编写许多mock和fake工具。他们甚至编写中大型集成测试。
```

2018-10-19 20:28:36

```
测试是应用产品的另外一种功能，而SET就是这个功能的负责人。
SET 与功能开发人员坐在一起（实际上，让他们物理位置坐在一起是也是我们的设计目标）。这样讲可能更公平一些，测试也是应用产品的一种功能特性，而 SET 是这个产品功能特性的负责人。SET参与SWE的代码评审，反之亦然。
```

2018-10-09 14:32:08
    为了能够尽早可以运行集成测试，针对依赖服务，SET 提供了 mock 与fake。

2018-10-19 20:28:47

```
在端到端自动化测试上过度投入，常常会把你与产品的特定功能设计绑定在一起。
在Google，SET遵循了下面的方法。
我们首先把容易出错的接口做隔离，并针对它们创建 mock 和 fake（在之前的章节中做过介绍），这样我们可以控制这些接口之间的交互，确保良好的测试覆盖率。
接下来构建一个轻量级的自动化框架，控制 mock 系统的创建和执行。这样的话，写代码的SWE可以使用这些mock接口来做一个私有构建。在他们把修改的代码提交到代码服务器之前运行相应的自动化测试，可以确保只有经过良好测试的代码才能被提交到代码库中。这是自动化测试擅长的地方，保证生态系统远离糟糕代码，并确保代码库永远处于一个时刻干净的状态。
```

2018-10-09 14:34:12
    
    为了使SET也成为源码的拥有者之一，Google把代码审查作为开发流程的中心。相比较编写代码而言，代码审查更值得炫耀。

2018-10-09 14:35:05

    小型测试是为了验证一个代码单元的功能。中型测试验证两个或多个模块应用之间的交互。大型测试是为了验证整个系统作为一个整体是如何工作的。

2018-10-19 20:29:49

```
小型测试带来优秀的代码质量、良好的异常处理、优雅的错误报告；大中型测试会带来整体产品质量和数据验证。
检验一个项目里小型测试、中型测试和大型测试之间的比率是否健康，一个好办法是使用代码覆盖率。测试代码覆盖率可以针对小型测试、中大型测试分别单独产生报告。覆盖率报告会针对不同的项目展示一个可被接受的覆盖率结果。如果中大型测试只有20%的代码覆盖率，而小型测试有近100%的覆盖率，则说明这个项目缺乏端到端的功能验证。如果结果数字反过来了，则说明这个项目很难去做升级扩展和维护，由于小型测试较少，就需要大量的时间消耗在底层代码调试查错上。
```

2018-10-19 20:29:22

```
Google有许多不同类型的项目，这些项目对测试的需求也不同，小型测试、中型测试和大型测试之间的比例随着项目团队的不同而不同。这个比例并不是固定的，总体上有一个经验法则，即70/20/10原则：70%是小型测试，20%是中型测试，10%是大型测试。
```


2018-10-06 12:06:44

    每个测试和其他测试之间都是独立的，使它们就能够以任意顺序来执行。

2018-10-06 12:06:56

    测试不做任何数据持久化方面的工作。

2018-10-06 12:18:10

    如果一个团队完成了一系列的测试任务，这个团队会得到一个通过“认证”的标识。所有团队最初的级别都是 0。如果掌握了基本的优秀代码习惯，就达到级别1，然后继续通过水平考核，最终达到级别5
2018-10-19 20:31:03

```
测试认证级别摘要
级别1
使用测试覆盖率工具。
使用持续集成。
测试分级为小型、中型、大型。
明确标记哪些测试是非确定性的测试（译注：非确定性测试指测试结果不确定的用例）。
创建冒烟测试集合。
级别2
如果有测试运行结果为红色（译注：表示运行失败的用例）就不会做发布。
在每次代码提交之前都要求通过冒烟测试。
各种类型测试的整体增量覆盖率要大于50%。
小型测试的增量覆盖率要大于10%。
每一个功能特性至少有一个与之对应的集成测试用例。
级别3
所有重要的代码变更都要经过测试。
小型测试的增量覆盖率要大于50%。
新增的重要功能都要经过集成测试的验证。
级别4
在提交任何新代码之前都会自动运行冒烟测试。
冒烟测试必须在30分钟内运行完毕。
没有不确定性的测试。
总体测试覆盖率应该不小于40%。
小型测试的代码覆盖率应该不小于25%。
所有重要的功能都应该被集成测试验证到。
级别5
对每一个重要的缺陷修复都要增加一个测试用例与之对应。
积极使用可用的代码分析工具。
总体测试覆盖率不低于60%。
小型测试的代码覆盖率应该不小于40%。
```

2018-10-06 13:07:09

```
ACC的指导原则如下。
  
  避免散漫的文字，推荐使用简明的列表。
  
  不必推销。
  
  简洁。

  不要把不重要的、无法执行的东西放进测试计划。

  渐进式的描述（Make it flow）。

  指导计划者的思路。

  最终结果应该是测试用例。
```

2018-10-06 13:21:55
ACC通过指导计划者依次考察产品的三个维度达成这个目标：描述产品目标的形容词和副词；确定产品各部分、各特性的名词；描述产品实际做什么的动词。这样，我们通过测试完成的就是验证这些能力（capabilities）能正常运作、产品各组件（component）能满足应用的目标。
1.A代表特质（Attribute）
特质是系统的形容词，代表了产品的品质和特色，是区别于竞争对手的关键。

2.C代表组件（component）
组件是系统的名词，在特质被识别之后确定。
组件是构成待建系统的模块
组件是使一个软件之所以如此的关键代码块。
他们正是测试人员要测试的对象

3.C代表能力（capability）
能力是系统的动词，代表着系统在用户指令之下完成的动作。它们是对输入的响应、对查询的应答，以及代表用户完成的活动。事实上，这正是用户选择一个软件的原因所在：他们需要一些功能而你的软件提供了这些功能。

能力处于特质和组件的交点。组件（component）执行某种功能（function）来满足产品的一个特质（attribute），这个活动的结果是向用户提供某种能力（capability）。

能力最重要的一个特点是它的可测试性。

一些能力应该满足的特性以供参考。
（1）一个能力点应当被表达为一个动作，反映了用户使用被测应用完成一定的活动。
（2）一个能力点应当为测试人员提供足够的指导，用以理解在编写测试用例时涉及的变量。
 (3）一个能力应当与其他能力组合。

ACC的完成，意味着所有可测试的特性都被定义好了，剩下的只是预算和时间的问题了。这就需要来排优先级
```

2018-10-07 11:04:10

    影响风险的因素很多，试图精确地、定量地计算风险比缓解风险还要麻烦。在Google，我们确定了两个要素：失败频率（frequency of failure）和影响（impact）。

2018-10-19 20:33:03

```
我们可以围绕风险大的能力点编写用户故事，并从中确定低风险的使用场景，然后反馈到开发团队，请他们有针对性地增加约束。
我们可以编写回归测试用例，以确保问题在重现时可以被捕捉到。
我们可以编写和运行引发故障的测试用例，来推动开发实现恢复和回滚的特性。
我们可以插入监听代码（instrumentation and watchdog code），以便更早地检测到故障。
我们可以插入代码监听软件，发现新旧版本间的行为变化以发现回归问题。
```

2018-10-19 20:33:11

    测试人员可能会参与到实际的缓解过程，但更主要的工作是暴露风险。

2018-10-07 11:07:37
    
    原则是：如果不能全测，就先测最重要的，也就是风险最大的。
2018-10-19 20:33:34

    知道A比B风险更大就足够了，不需要过分关心它们的具体风险值。
2018-10-19 20:34:02
（1）对于任何在GTA矩阵中显示为红色的高风险的能力点和特质—组件对，一定要编写一系列用户故事、用例或者有针对性的测试指导。
 (2）认真了解之前已经完成的面向SET和SWE的测试，评估这些测试对GTA所暴露的风险级别的影响。这些测试是否足够了？还需要增加额外的测试吗？TE需要自己编写一些测试用例，还是需要请SET或SWE来编写。重点不在于谁来写，而在于有人写。
（3）分析每个高风险的特质—能力对相关的bug，保证回归测试用例存在。
（4）仔细思索高风险的区域，咨询可能的回滚和恢复机制。
（5）引入尽可能多的相关各方。
（6）如果以上措施皆不奏效，某个高风险的组件仍然处于测试不足的状态，经常出问题，那就得非常努力地去游说相关同事。
```

2018-10-07 11:14:21

```
用户故事描述了真实用户或期望用户使用被测应用的行为，描述了用户的动机和视角，而有意忽略产品的实现和设计细节。

在编写用户故事的时候，我们仅从用户界面角度出发关注产品，而绝不应该描述技术性内容。

用户故事的焦点在于对用户的价值，而测试用例则要比用户故事更加具体，测试用例通常指定了具体的输入和输出。
```

2018-10-07 11:19:33
    
    对于风险较低的能力点，可以降低些要求。我们可能会做出判断：为较低风险的领域编写特定的测试用例是得不偿失的。我们可能会选择进行探索式测试，或者使用众包去测试这些领域。
2018-10-07 11:20:18
    
    众包是测试领域的一个新现象，它的产生基于以下事实：测试人员在数量上很少，而且拥有的资源也很有限；但用户则为数众多，而且拥有每一种我们希望用来测试应用的硬件和环境组合。当然会有一部分用户愿意帮忙，对不对？
2018-10-07 11:22:56

    ACC的真正威力在于它能用来确定一系列的能力点，按风险排序，然后分配给所有的质量伙伴。

2018-10-08 11:36:19

    BITE代表Browser Integrated Test Environment（浏览器集成测试环境），目标是把尽可能多的测试活动、测试工具和测试数据集中到浏览器和云里，并在上下文中呈现相关信息，从而减少分散操作的麻烦，使得测试工作更高效。

2018-10-08 11:37:27

BITE会自动完成一些很酷的事情，使得后续的bug triage和调试容易得多。

 (1）自动截图，保存为bug的附件。
（2）高亮元素的HTML被附加到bug。
（3）从打开“maps.google.com”开始的所有动作都会被录制下来，保存为一份JavaScript脚本，将来可以回放以重现 bug。
（4）Map特定的调试URL被自动地附加到bug上，因为网页本身的URL没有足够的信息用于完整的重现。
（5）所有浏览器和OS信息被附加到bug。
这一切成就了 bug 的快速提交，并且提交到数据库中的 bug 有足够多的信息用于安排优先级。

使用BITE查看bug

使用BITE进行录制和回放.实现了一个称为Record and Playback framework（RPF）的纯Web的解决方案，是用纯 JavaScript 实现的，并将测试用例脚本保存在云端。

Google有句名言“资源越少，目标越明了（scarcity brings clarity）”

使用BITE执行手工和探索式测试

 BITE支持测试人员订阅Google Test Case Manager（GTCM）中多个产品的测试集。测试主管在安排测试时，只需要在BITE服务器上点击一个按钮，就可以把测试用例通过BITE的UX推送到各个测试人员。每个测试都有一个关联的URL。用户接受任务之后，BITE 会打开该 URL 并显示要在当前页面执行的测试步骤和验证。运行结束时，单击标记PASS，系统自动打开下一个要测试的URL。如果测试失败，系统就做FAIL标记并启动BITE的bug报告界面。

BITE的分层化设计
```

2018-10-19 20:37:27
```
Lindsay：对于一个新项目，我首先要站在用户的角度了解这个产品。有可能的话，我会作为一个用户，以自己的账户和个人数据去使用产品。我努力使自己经历完整的用户体验。一旦有自己的真实数据在里面，你对一个产品的期待会彻底改变。在具备了用户心态之后，我会做下面的一些事情。
从头到尾的理解产品。不管是整体的设计文档，还是主要功能的设计文档，我都会去看。只要有文档，我就看。
在消化了这些文档之后，我开始关注项目的状态，特别是质量状态。我会去了解bug数量、问题的分组方式、已经报告的bug类型、最长时间未处理的bug、最近一些bug的类型等，我还会看一下发现—修复比例。
HGTS：是按照每个开发人员还是整个团队？
Lindsay：都有！不夸张地说，只有熟悉了团队的全貌，才能真正有效的展开工作。
我还会去检查应用的代码库。对每一个大一点的类，我会寻找关联的单元测试，并且运行这些测试查看是否能够通过。这些测试用例是否有效？是否完整？有集成或端到端的测试用例吗？它们仍然通过吗？历史的通过率是多少？这些测试用例只是基本场景，还是也覆盖到了边界情况？代码库的哪些包变化最多？哪些已经很长时间没有变更了？开发人员在测试方面的文档工作是否非常随意。
我还会评审所有自动化测试。有自动化测试吗？是否还在运行且能运行通过吗？不管怎样，我都要去检查测试代码，理解每个测试步骤，看它们是否完整，看相关的假设、通过和失败点是否正确、是否有效。有时，自动化只覆盖了简单的测试；有时，自动化测试集包含了复杂的用户场景（这是一个非常好的迹象）。
在看完所有文档之后，接下来是团队。我会了解他们沟通的方式和对测试人员的期望。如果他们使用电子邮件列表，我会全部加入；如果有团队IRC或其他的实时通讯方式，我也会加入。
询问他们对测试的期望，会帮助发现开发团队没有测试过的内容。


开始干正事了。第一件事是把应用分解为合理的功能模块，有一点重叠没有关系。分解不能太细，以免纠缠于细节；但也不能太粗，必须细致到可以罗列子模块和功能。
有了功能模块，就可以排列测试的优先级了。风险最大的是哪部分呢？
到这里，我会再次检查bug库。这次是按模块对bug进行分组。这将加快已有bug的查找，减少重复的bug，更容易暴露不断重现的问题。
接下来，我会按照优先级顺序更加细致地遍历所有模块，创建用户故事（译注：user story）。对于那些需要详细的步骤说明才能决定pass/fail的特性，通常会编写测试用例并链接到相应模块的用户故事。针对比较奇怪的 bug，尽量附加上屏幕截图、视频、快速参考，或者指向现存bug的链接。
有了测试集合，我接下来会通过再次检查bug和应用来寻找覆盖度上的不足。测试人员做的很多事情是周期性的。此时，我会查看不同类型的测试，检查覆盖情况：安全、兼容性、集成、探索式的、回归、性能、负载等。
有了这些基础材料，我的工作通常只是维护和更新：更新测试用例，增加新特性的文档，更新变化了的模块的截屏或视频。最后，观察哪些bug遗漏到了生产环境，会告诉我们测试覆盖上的不足。
```

2018-10-19 20:38:20

```
"money tour"（侧重于金钱相关的特性；对YouTube而言是与广告或合作伙伴相关的特性）得到很大关注，对于各个发布都很重要。“landmark tour”（侧重于重要的系统功能和特性）和“bad neighborhood tour”（侧重于先前的重灾区和基于最近出现的bug确定的问题突出领域）在发现最严重的 bug 方面最为有效。

旅游的概念对我们解释和分享探索式测试的策略非常有用。我们还饶有兴趣的拿一些漫游模式开玩笑，例如“antisocial tour”（一有机会就输入最不可能的输入）、“obsessive compulsive tour”（重复同一动作）和“couch potato tour”（只给最小输入、接受一切默认值）。

关于使用Selenium实现测试自动化，你最喜欢的和最不喜欢的分别是什么？
Apple：最喜欢的是简单的API，你可以使用自己喜欢的编程语言写测试代码，如Python、Java和Ruby。你可以从应用里直接调用JavaScript代码这种了不起的特性非常有用。
最不喜欢的依旧是浏览器测试。运行速度较慢，还需要API回调，测试离被测的对象很远。它有助于通过自动化那些人工验证极端困难的场景来保证产品质量，如对广告系统后端的调用。我们有一些测试用于加载不同的视频，然后使用Banana Proxy（一个内部开发的Web应用安全审计工具，可以记录HTTP请求和应答）拦截广告调用。这样，浏览器请求经过Banana Proxy（日志）的中转到达Selenium，再到达Web。由此，我们可以检查向外发送的请求是否包含了正确的URL参数，接收到的响应是否包含了期望的内容。总的来说，UI 测试的执行比较慢，非常脆弱，维护成本也很高。一个教训是，只保留少数几个用来验证端到端的集成场景的高级别冒烟测试，除此之外尽可能编写底层的测试用例。

不管是测试框架还是测试用例都以简单为要，随着项目的开展再迭代的设计。不要试图事先解决所有问题。要敢于扔掉过时的东西。如果测试或者自动化过于难以维护，不如放弃它并试着去实现更有韧性、更好的东西。密切关注一段时间维护和排错的成本。遵守70-20-10法则：小型的用来验证单个类或功能的单元测试占70%，中型的用来验证一个或多个应用模块之间集成的测试占20%，大型的高级别的用来验证完整应用的测试（一般称为系统测试和端到端测试）占10%。
除此之外，安排好优先级，寻找小成本大回报的自动化项目。一定要记住自动化并不能解决所有问题，尤其是前端项目和设备测试。你总会需要聪明的、探索式的测试并跟踪测试数据。

```

2018-10-19 20:40:50
     
     跨团队的交流必须建立在创新的基础之上，否则就是浪费时间。

2018-10-09 11:51:04

```
多年来，通过不断地聆听，我发现最有力的问题就是“为什么”。为什么你会进行这些测试？为什么你会想到这个用例？为什么你选择把这个任务自动化而不是那个任务？为什么我们要投入做这个工具？
```

2018-10-09 11:57:21
    
    我还是坚信只应该关注最重要的事情。每当我发现团队打算做太多的东西的时候，就好像你要同时做五件事情，但是每件只能完成 80%的时候，我就会要求他们退回来重新安排优先级。把你需要做的事情减少到两到三件，但都能完成到100%。这样团队才能获得真正的成就感，而不是好多事情在他们手里没有完成。
2018-10-09 11:58:31
    
    人员的问题其实很简单，那就是绝不妥协。选用不合适的人来填充名额永远要比等待合适的人员要糟糕。只选用最好的人，不能动摇。
2018-10-19 20:41:09

```
使用与应用程序开发语言相同的编程语言来编写测试。
让负责开发新特性的人同时负责相应测试的执行，他需要对漏掉的测试负责。
关注测试基础设施的建设，让测试的编写和执行非常容易，甚至比忽略它们还要容易。
20%的用例覆盖了80%的使用场景（可能会有些出入）。把这20%自动化而别管剩下的。把那些测试通过手工完成。
```

2018-10-19 20:41:17
```
假设我们知道用户的需求，然后进行了大规模的改动或编写了大量的代码提供新特性，却没有进行小规模的试验。如果用户不喜欢这些改动，麻烦就大了，而针对这些特性构造的测试框架再好也是浪费。因此，要先为少量用户放出一个版本，获得必要的反馈，然后再为大量的自动化测试进行投资。
```

2018-10-09 12:02:18

    测试开发工程师应该牢记测试应该是开发人员的工作而他们自己应该专心让测试成为开发人员工作中的一环。我们通过编写工具帮助开发人员做到这点，而且应该让开发人员在维护开发代码的同时也负责维护测试代码。
2018-10-19 20:41:33
    
    JavaScript自动化测试。我们为Gmail本身加入了一个用于自动化测试的servlet。通过它，开发人员就可以使用与前端开发一致的编程语言编写端到端的测试（译注：端到端的测试是指涉及整个应用系统环境，在现实世界使用时的情形模拟的测试。）。

2018-10-09 12:16:52

    团队建立好以后，我给他们定下了基调：创造价值！最好还能找到可复制的创造价值的方法。从开发到产品管理，测试都应该是一股推动的力量，否则你就是在阻碍发展
2018-10-09 12:18:33
    
    自动化测试需要能够快速编写、快速执行、解决特定的问题。如果你不能立即理解一个自动化测试用例的目的，那就说明这个用例太复杂了。你需要让自动化测试足够简单，有确定的范围，最重要的是要产生价值。
2018-10-19 20:41:59

```
HGTS：你会为手工测试创建文档吗？还是纯粹的探索式测试？
Hung：是探索式的，但也为它创建文档。在两种情况下，我们会为手工测试创建文档。第一种情况是，当我们有一个通用的用例，它可以被每次的构建版本使用而且也是主要的测试路径。我们把这些用例记录下来放到GTCM中，这样所有的测试人员或测试外包都能获取和使用这些文档。第二种情况是，我们为每个功能点记录测试指导方案。每个功能点都有自己的特性。手工测试人员把这些特性作为一些指导原则记录下来，一般后来的测试人员能够在后续的版本里很快接手这个功能点的测试。总的来说，我们会花时间为系统级的用例和特定于功能点的指导方针创建文档。
```


2018-10-09 13:29:12

```
Pat Copeland给了我两条建议。第一条，是先花一些时间来观察学习。
他建议我选择发布一些重大的产品，可能的话，做出些与众不同的东西来。
先虚心学习，再在一线作出成绩，然后开始寻求创新的方法。
```

2018-10-19 20:42:53

    
    测试人员所拥有的技术能力（包括计算机科学的专业文凭）、测试资源的稀缺从而获得开发人员帮助和不断进行测试优化、优先考虑自动化（这样才能让人去做那些计算机做不好的事情），以及快速迭代、集成和获得用户反馈的能力。其他公司要想效仿Google的做法，应该从这四个方面做起：技能、稀缺性、自动化和迭代集成。这就是Google测试的“秘方”，照方抓药吧！

2018-10-19 20:43:09
    
    Google的测试流程可以非常简练地概括为：让每个工程师都注重质量。只要大家诚实认真地这么做，质量就会提高。代码质量从一开始就能更好，早期构建版本的质量会更高，集成也不再是必须的，系统测试可以关注于真正面向用户的问题。所有的工程师和项目都能从堆积如山的bug中解脱出来。

2018-10-19 20:43:38

```
在过去，软件每周或每月构建一次并需要经历痛苦的集成过程，特别需要测试人员能够发现bug，并尽可能地模仿最终用户的操作。产品交付以后有数百万的最终用户使用，问题很难被跟踪，产品也没办法及时更新，因此必须在产品交付之前发现 bug。可现在已经不是这样了。通过互联网交付软件，意味着我们有能力选择部分用户进行发布，响应这部分用户的反馈，并迅速进行更新。开发者和最终用户之间沟通合作的障碍不复存在。bug 的寿命从几个月变成了几分钟。我们非常快速地进行构建、交付（给内部试用者、可信赖的测试者、早期用户或真实用户）、修改、重新迭代交付，让很多用户根本来不及发现缺陷。这是一种更好的软件交付和用户反馈机制。
```